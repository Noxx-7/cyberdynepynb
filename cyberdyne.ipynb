{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ü§ñ Cyberdyne LLM - Complete Training & Inference System\n\nA complete system for training and deploying custom language models with ChatGPT-like capabilities.\n\n## Features\n- üéì Train custom language models from scratch\n- üìä Multi-dataset support (9+ Hugging Face datasets)\n- üíæ Offline inference\n- üí¨ ChatGPT-style conversational interface\n- üé® Gradio UI for easy interaction\n\n## Quick Start Guide\n1. Run the installation cell\n2. Choose to either train a new model or load an existing one\n3. Use the chat interface to interact with your model\n\n**Compatible with Google Colab & Kaggle**","metadata":{}},{"cell_type":"markdown","source":"## üì¶ Installation & Setup","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q torch>=2.0.0 transformers>=4.30.0 datasets>=2.14.0 gradio>=4.0.0 tqdm>=4.65.0 huggingface_hub>=0.16.0 accelerate>=0.20.0\n\nprint(\"‚úÖ All packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:39:44.340297Z","iopub.execute_input":"2025-10-24T13:39:44.340823Z","iopub.status.idle":"2025-10-24T13:41:09.350759Z","shell.execute_reply.started":"2025-10-24T13:39:44.340800Z","shell.execute_reply":"2025-10-24T13:41:09.349693Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ All packages installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport time\nimport os\nimport math\nimport uuid\nfrom datetime import datetime\nfrom typing import Optional, Dict, List\nimport gradio as gr\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"üñ•Ô∏è  Using device: {device}\")\nif device == 'cuda':\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:41:09.352513Z","iopub.execute_input":"2025-10-24T13:41:09.352778Z","iopub.status.idle":"2025-10-24T13:41:20.878175Z","shell.execute_reply.started":"2025-10-24T13:41:09.352755Z","shell.execute_reply":"2025-10-24T13:41:20.877356Z"}},"outputs":[{"name":"stdout","text":"üñ•Ô∏è  Using device: cuda\n   GPU: Tesla T4\n   Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## üèóÔ∏è Model Architecture\n\nDecoder-only Transformer (GPT-style) with:\n- Multi-head self-attention\n- Layer normalization with residual connections\n- Configurable depth and width","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, emb_size, n_heads, dropout=0.1):\n        super().__init__()\n        assert emb_size % n_heads == 0\n\n        self.emb_size = emb_size\n        self.n_heads = n_heads\n        self.head_dim = emb_size // n_heads\n\n        self.qkv = nn.Linear(emb_size, 3 * emb_size)\n        self.out = nn.Linear(emb_size, emb_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        out = torch.matmul(attn_weights, v)\n        out = out.permute(0, 2, 1, 3).contiguous()\n        out = out.reshape(batch_size, seq_len, self.emb_size)\n        out = self.out(out)\n\n        return out\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, emb_size, ff_size, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(emb_size, ff_size)\n        self.fc2 = nn.Linear(ff_size, emb_size)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, emb_size, n_heads, ff_size, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(emb_size, n_heads, dropout)\n        self.ff = FeedForward(emb_size, ff_size, dropout)\n        self.ln1 = nn.LayerNorm(emb_size)\n        self.ln2 = nn.LayerNorm(emb_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_out = self.attn(self.ln1(x), mask)\n        x = x + self.dropout(attn_out)\n        ff_out = self.ff(self.ln2(x))\n        x = x + self.dropout(ff_out)\n        return x\n\n\nclass AdvancedLLM(nn.Module):\n    def __init__(self, vocab_size, emb_size=768, n_layers=12, n_heads=12,\n                 ff_size=3072, max_len=512, dropout=0.1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.max_len = max_len\n\n        self.token_embed = nn.Embedding(vocab_size, emb_size)\n        self.pos_embed = nn.Embedding(max_len, emb_size)\n        self.dropout = nn.Dropout(dropout)\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(emb_size, n_heads, ff_size, dropout)\n            for _ in range(n_layers)\n        ])\n\n        self.ln_final = nn.LayerNorm(emb_size)\n        self.head = nn.Linear(emb_size, vocab_size, bias=False)\n\n        self.token_embed.weight = self.head.weight\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    torch.nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len = x.shape\n\n        token_emb = self.token_embed(x)\n        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n        pos_emb = self.pos_embed(positions)\n\n        x = self.dropout(token_emb + pos_emb)\n\n        for block in self.blocks:\n            x = block(x, mask)\n\n        x = self.ln_final(x)\n        logits = self.head(x)\n\n        return logits\n\n    def get_num_params(self):\n        return sum(p.numel() for p in self.parameters())\n\n    def save_model(self, path):\n        config = {\n            'vocab_size': self.vocab_size,\n            'emb_size': self.emb_size,\n            'n_layers': self.n_layers,\n            'n_heads': self.n_heads,\n            'max_len': self.max_len,\n            'state_dict': self.state_dict()\n        }\n        torch.save(config, path)\n        print(f\"‚úÖ Model saved to {path}\")\n\n    @classmethod\n    def load_model(cls, path, device='cpu'):\n        config = torch.load(path, map_location=device)\n        model = cls(\n            vocab_size=config['vocab_size'],\n            emb_size=config['emb_size'],\n            n_layers=config['n_layers'],\n            n_heads=config['n_heads'],\n            max_len=config['max_len']\n        )\n        model.load_state_dict(config['state_dict'])\n        model.to(device)\n        print(f\"‚úÖ Model loaded from {path}\")\n        return model\n\nprint(\"‚úÖ Model architecture defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:41:20.879301Z","iopub.execute_input":"2025-10-24T13:41:20.880327Z","iopub.status.idle":"2025-10-24T13:41:20.908954Z","shell.execute_reply.started":"2025-10-24T13:41:20.880296Z","shell.execute_reply":"2025-10-24T13:41:20.908269Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model architecture defined\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üìö Dataset Loader\n\nSupports multiple Hugging Face datasets:\n- General: wikitext, wikipedia, openwebtext, bookcorpus, c4, pile\n- Instruction: dolly, alpaca, squad","metadata":{}},{"cell_type":"code","source":"class HuggingFaceDataset(IterableDataset):\n    def __init__(self, dataset_name, tokenizer, max_samples=100000,\n                 max_len=512, split='train', config=None, streaming=True,\n                 text_field='text', instruction_field=None):\n        self.dataset_name = dataset_name\n        self.tokenizer = tokenizer\n        self.max_samples = max_samples\n        self.max_len = max_len\n        self.split = split\n        self.config = config\n        self.streaming = streaming\n        self.text_field = text_field\n        self.instruction_field = instruction_field\n\n        try:\n            if config:\n                self.dataset = load_dataset(dataset_name, config, split=split, streaming=streaming)\n            else:\n                self.dataset = load_dataset(dataset_name, split=split, streaming=streaming)\n            print(f\"‚úÖ Loaded dataset: {dataset_name}\")\n        except Exception as e:\n            print(f\"‚ùå Error loading dataset {dataset_name}: {e}\")\n            raise\n\n    def __iter__(self):\n        count = 0\n        for item in self.dataset:\n            if count >= self.max_samples:\n                break\n\n            text = self._extract_text(item)\n            if not text:\n                continue\n\n            tokens = self.tokenizer.encode(\n                text,\n                max_length=self.max_len,\n                truncation=True,\n                padding='max_length',\n                return_tensors='pt'\n            ).squeeze(0)\n\n            target = tokens.clone()\n            target[:-1] = tokens[1:]\n            target[-1] = self.tokenizer.eos_token_id\n\n            yield tokens, target\n            count += 1\n\n    def _extract_text(self, item):\n        if self.instruction_field and self.instruction_field in item:\n            if isinstance(item[self.instruction_field], list):\n                instruction = item[self.instruction_field][0] if item[self.instruction_field] else \"\"\n            else:\n                instruction = item[self.instruction_field]\n\n            if self.text_field in item:\n                if isinstance(item[self.text_field], list):\n                    response = item[self.text_field][0] if item[self.text_field] else \"\"\n                else:\n                    response = item[self.text_field]\n                return f\"Instruction: {instruction}\\n\\nResponse: {response}\"\n            return instruction\n\n        if self.text_field in item:\n            text = item[self.text_field]\n            if isinstance(text, list):\n                return text[0] if text else \"\"\n            return text\n\n        for key in ['text', 'content', 'document', 'article', 'response', 'output']:\n            if key in item:\n                value = item[key]\n                if isinstance(value, list):\n                    return value[0] if value else \"\"\n                return value\n\n        return \"\"\n\n\nclass MultiDatasetLoader:\n    DATASET_CONFIGS = {\n        'wikipedia': {\n            'name': 'wikipedia',\n            'config': '20231101.en',\n            'text_field': 'text',\n        },\n        'openwebtext': {\n            'name': 'openwebtext',\n            'text_field': 'text',\n        },\n        'wikitext': {\n            'name': 'wikitext',\n            'config': 'wikitext-103-v1',\n            'text_field': 'text',\n        },\n        'bookcorpus': {\n            'name': 'bookcorpus',\n            'text_field': 'text',\n        },\n        'c4': {\n            'name': 'c4',\n            'config': 'en',\n            'text_field': 'text',\n        },\n        'dolly': {\n            'name': 'databricks/databricks-dolly-15k',\n            'text_field': 'response',\n            'instruction_field': 'instruction',\n        },\n        'alpaca': {\n            'name': 'tatsu-lab/alpaca',\n            'text_field': 'output',\n            'instruction_field': 'instruction',\n        },\n        'squad': {\n            'name': 'squad',\n            'text_field': 'context',\n        },\n        'pile': {\n            'name': 'EleutherAI/pile',\n            'text_field': 'text',\n        },\n    }\n\n    @classmethod\n    def create_dataset(cls, dataset_key, tokenizer, max_samples=100000,\n                       max_len=512, split='train', streaming=True):\n        if dataset_key not in cls.DATASET_CONFIGS:\n            raise ValueError(f\"Unknown dataset: {dataset_key}. Available: {list(cls.DATASET_CONFIGS.keys())}\")\n\n        config = cls.DATASET_CONFIGS[dataset_key]\n        return HuggingFaceDataset(\n            dataset_name=config['name'],\n            tokenizer=tokenizer,\n            max_samples=max_samples,\n            max_len=max_len,\n            split=split,\n            config=config.get('config'),\n            streaming=streaming,\n            text_field=config['text_field'],\n            instruction_field=config.get('instruction_field')\n        )\n\n    @classmethod\n    def list_available_datasets(cls):\n        return list(cls.DATASET_CONFIGS.keys())\n\n    @classmethod\n    def get_dataset_info(cls, dataset_key):\n        if dataset_key in cls.DATASET_CONFIGS:\n            return cls.DATASET_CONFIGS[dataset_key]\n        return None\n\nprint(\"‚úÖ Dataset loader defined\")\nprint(f\"üìö Available datasets: {MultiDatasetLoader.list_available_datasets()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:41:20.910683Z","iopub.execute_input":"2025-10-24T13:41:20.910853Z","iopub.status.idle":"2025-10-24T13:41:20.932024Z","shell.execute_reply.started":"2025-10-24T13:41:20.910838Z","shell.execute_reply":"2025-10-24T13:41:20.931369Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dataset loader defined\nüìö Available datasets: ['wikipedia', 'openwebtext', 'wikitext', 'bookcorpus', 'c4', 'dolly', 'alpaca', 'squad', 'pile']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üéì Training Engine","metadata":{}},{"cell_type":"code","source":"class LLMTrainer:\n    def __init__(self, model_name='cyberdyne-llm', vocab_size=None, emb_size=768,\n                 n_layers=12, n_heads=12, ff_size=3072, max_len=512,\n                 learning_rate=5e-5, device=None):\n        self.model_name = model_name\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n\n        if vocab_size is None:\n            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n            vocab_size = len(tokenizer)\n\n        self.model = AdvancedLLM(\n            vocab_size=vocab_size,\n            emb_size=emb_size,\n            n_layers=n_layers,\n            n_heads=n_heads,\n            ff_size=ff_size,\n            max_len=max_len\n        ).to(self.device)\n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n        print(f\"ü§ñ Model initialized with {self.model.get_num_params():,} parameters\")\n        print(f\"üñ•Ô∏è  Using device: {self.device}\")\n\n    def train(self, dataset_key, num_epochs=3, batch_size=4, max_samples=10000,\n              max_len=512, save_dir='models', log_interval=10):\n        print(f\"\\nüéì Starting training on dataset: {dataset_key}\")\n        print(f\"   Epochs: {num_epochs}, Batch size: {batch_size}, Max samples: {max_samples}\")\n\n        os.makedirs(save_dir, exist_ok=True)\n\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n        try:\n            dataset = MultiDatasetLoader.create_dataset(\n                dataset_key,\n                tokenizer,\n                max_samples=max_samples,\n                max_len=max_len,\n                streaming=True\n            )\n        except Exception as e:\n            print(f\"‚ùå Error loading dataset: {e}\")\n            return None\n\n        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)\n\n        training_start = time.time()\n        training_history = {\n            'model_name': self.model_name,\n            'dataset': dataset_key,\n            'epochs': num_epochs,\n            'batch_size': batch_size,\n            'losses': [],\n            'epoch_losses': []\n        }\n\n        self.model.train()\n\n        for epoch in range(num_epochs):\n            epoch_loss = 0.0\n            batch_count = 0\n\n            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n            for batch_idx, (inputs, targets) in enumerate(progress_bar):\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n\n                self.optimizer.zero_grad()\n\n                logits = self.model(inputs)\n                loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n\n                epoch_loss += loss.item()\n                batch_count += 1\n\n                if batch_idx % log_interval == 0:\n                    progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n                    training_history['losses'].append({\n                        'epoch': epoch + 1,\n                        'batch': batch_idx,\n                        'loss': loss.item()\n                    })\n\n            avg_epoch_loss = epoch_loss / batch_count\n            training_history['epoch_losses'].append(avg_epoch_loss)\n            print(f\"‚úÖ Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n\n            checkpoint_path = os.path.join(save_dir, f\"{self.model_name}_epoch_{epoch+1}.pt\")\n            self.model.save_model(checkpoint_path)\n\n        training_duration = time.time() - training_start\n        training_history['duration'] = training_duration\n\n        final_path = os.path.join(save_dir, f\"{self.model_name}_final.pt\")\n        self.model.save_model(final_path)\n\n        print(f\"\\nüéâ Training completed in {training_duration/60:.2f} minutes\")\n        print(f\"üíæ Final model saved to: {final_path}\")\n\n        return training_history\n\nprint(\"‚úÖ Training engine defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:41:20.932798Z","iopub.execute_input":"2025-10-24T13:41:20.933030Z","iopub.status.idle":"2025-10-24T13:41:20.952864Z","shell.execute_reply.started":"2025-10-24T13:41:20.933010Z","shell.execute_reply":"2025-10-24T13:41:20.952200Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training engine defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## üí¨ Inference Engine\n\nConversational AI with context management and multiple generation parameters","metadata":{}},{"cell_type":"code","source":"class ConversationalInference:\n    def __init__(self, model, tokenizer_name='gpt2', device=None, max_context_length=5):\n        self.model = model\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n        self.max_context_length = max_context_length\n        self.conversations = {}\n\n        print(f\"üí¨ Inference engine initialized on {self.device}\")\n\n    def create_session(self):\n        session_id = str(uuid.uuid4())\n        self.conversations[session_id] = []\n        return session_id\n\n    def get_conversation_history(self, session_id):\n        return self.conversations.get(session_id, [])\n\n    def clear_session(self, session_id):\n        if session_id in self.conversations:\n            self.conversations[session_id] = []\n\n    def generate_response(self, prompt, session_id=None, max_new_tokens=150,\n                         temperature=0.8, top_k=50, top_p=0.95,\n                         use_context=True):\n        if session_id is None:\n            session_id = self.create_session()\n\n        if session_id not in self.conversations:\n            self.conversations[session_id] = []\n\n        if use_context and self.conversations[session_id]:\n            context_messages = self.conversations[session_id][-self.max_context_length:]\n            context_text = self._build_context(context_messages)\n            full_prompt = f\"{context_text}\\nUser: {prompt}\\nAssistant:\"\n        else:\n            full_prompt = f\"User: {prompt}\\nAssistant:\"\n\n        response = self._generate_text(\n            full_prompt,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p\n        )\n\n        self.conversations[session_id].append({\n            'role': 'user',\n            'content': prompt\n        })\n        self.conversations[session_id].append({\n            'role': 'assistant',\n            'content': response\n        })\n\n        return response, session_id\n\n    def _build_context(self, messages):\n        context_parts = []\n        for msg in messages:\n            if msg['role'] == 'user':\n                context_parts.append(f\"User: {msg['content']}\")\n            else:\n                context_parts.append(f\"Assistant: {msg['content']}\")\n        return \"\\n\".join(context_parts)\n\n    def _generate_text(self, prompt, max_new_tokens=150, temperature=0.8,\n                       top_k=50, top_p=0.95):\n        tokens = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n\n        if tokens.size(1) > self.model.max_len - max_new_tokens:\n            tokens = tokens[:, -(self.model.max_len - max_new_tokens):]\n\n        generated = tokens\n\n        with torch.no_grad():\n            for _ in range(max_new_tokens):\n                if generated.size(1) >= self.model.max_len:\n                    break\n\n                logits = self.model(generated)\n                next_token_logits = logits[:, -1, :]\n\n                next_token_logits = next_token_logits / temperature\n\n                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n                next_token_logits[indices_to_remove] = float('-inf')\n\n                top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), top_k)\n                next_token = top_indices[0, torch.multinomial(top_probs[0], 1)]\n\n                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n\n\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        output_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n\n        if \"Assistant:\" in output_text:\n            response = output_text.split(\"Assistant:\")[-1].strip()\n        else:\n            response = output_text[len(prompt):].strip()\n\n        return response\n\n    def chat(self, message, session_id=None, **kwargs):\n        return self.generate_response(message, session_id=session_id, **kwargs)\n\n\nclass OfflineInference:\n    def __init__(self, model_path, device=None):\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n\n        print(f\"üìÇ Loading model from: {model_path}\")\n        checkpoint = torch.load(model_path, map_location=self.device)\n\n        self.model = AdvancedLLM(\n            vocab_size=checkpoint['vocab_size'],\n            emb_size=checkpoint['emb_size'],\n            n_layers=checkpoint['n_layers'],\n            n_heads=checkpoint['n_heads'],\n            max_len=checkpoint['max_len']\n        )\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.model.to(self.device)\n\n        tokenizer_name = checkpoint.get('tokenizer_name', 'gpt2')\n        self.inference_engine = ConversationalInference(\n            self.model,\n            tokenizer_name=tokenizer_name,\n            device=self.device\n        )\n\n        print(\"‚úÖ Offline inference ready!\")\n\n    def chat(self, message, session_id=None, **kwargs):\n        return self.inference_engine.chat(message, session_id=session_id, **kwargs)\n\n    def create_session(self):\n        return self.inference_engine.create_session()\n\n    def clear_session(self, session_id):\n        self.inference_engine.clear_session(session_id)\n\n    def get_history(self, session_id):\n        return self.inference_engine.get_conversation_history(session_id)\n\nprint(\"‚úÖ Inference engine defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:03:30.666924Z","iopub.execute_input":"2025-10-24T16:03:30.667776Z","iopub.status.idle":"2025-10-24T16:03:30.695372Z","shell.execute_reply.started":"2025-10-24T16:03:30.667741Z","shell.execute_reply":"2025-10-24T16:03:30.694553Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Inference engine defined\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## üöÄ Quick Training Example\n\nTrain a small model for testing (recommended for first run)","metadata":{}},{"cell_type":"code","source":"# Create models directory\nos.makedirs('models', exist_ok=True)\n\n# Initialize trainer with small configuration for quick training\ntrainer = LLMTrainer(\n    model_name='cyberdyne-quickstart',\n    emb_size=512,\n    n_layers=6,\n    n_heads=8,\n    learning_rate=5e-5\n)\n\n# Train on wikitext dataset\nhistory = trainer.train(\n    dataset_key='wikitext',\n    num_epochs=2,\n    batch_size=4,\n    max_samples=5000,\n    save_dir='models'\n)\n\nif history:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä Training Summary\")\n    print(\"=\"*60)\n    print(f\"Model: {history['model_name']}\")\n    print(f\"Dataset: {history['dataset']}\")\n    print(f\"Epochs: {history['epochs']}\")\n    print(f\"Final Loss: {history['epoch_losses'][-1]:.4f}\")\n    print(f\"Duration: {history['duration']/60:.2f} minutes\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:41:20.982363Z","iopub.execute_input":"2025-10-24T13:41:20.982588Z","iopub.status.idle":"2025-10-24T13:52:01.124689Z","shell.execute_reply.started":"2025-10-24T13:41:20.982572Z","shell.execute_reply":"2025-10-24T13:52:01.124026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a81d6e728044d4b1f729c80e4cda48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"756a2cfe632742cca72d98e1873a785b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae01cd8abb2f456ebb04e736eac5e253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6c6d379211640f7b0605a35d47b46fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ed9e6df7e24814832e7b5d8f2ade36"}},"metadata":{}},{"name":"stdout","text":"ü§ñ Model initialized with 51,207,168 parameters\nüñ•Ô∏è  Using device: cuda\n\nüéì Starting training on dataset: wikitext\n   Epochs: 2, Batch size: 4, Max samples: 5000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917a0105e613439e80c23e6f0e4a65df"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Loaded dataset: wikitext\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2: 1250it [05:08,  4.05it/s, loss=1.5690]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 1 completed. Average loss: 1.4418\n‚úÖ Model saved to models/cyberdyne-quickstart_epoch_1.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 1250it [05:20,  3.89it/s, loss=1.4624]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 2 completed. Average loss: 1.2092\n‚úÖ Model saved to models/cyberdyne-quickstart_epoch_2.pt\n‚úÖ Model saved to models/cyberdyne-quickstart_final.pt\n\nüéâ Training completed in 10.51 minutes\nüíæ Final model saved to: models/cyberdyne-quickstart_final.pt\n\n============================================================\nüìä Training Summary\n============================================================\nModel: cyberdyne-quickstart\nDataset: wikitext\nEpochs: 2\nFinal Loss: 1.2092\nDuration: 10.51 minutes\n============================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üíæ Advanced Training Configuration\n\nCustomize your model architecture and training parameters","metadata":{}},{"cell_type":"code","source":"# Advanced training configuration\nadvanced_trainer = LLMTrainer(\n    model_name='cyberdyne-advanced',\n    emb_size=768,\n    n_layers=12,\n    n_heads=12,\n    ff_size=3072,\n    learning_rate=5e-5\n)\n\n# Train on instruction dataset for better chat performance\nadvanced_history = advanced_trainer.train(\n    dataset_key='dolly',\n    num_epochs=3,\n    batch_size=8,\n    max_samples=20000,\n    save_dir='models'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T13:52:01.125485Z","iopub.execute_input":"2025-10-24T13:52:01.126104Z","iopub.status.idle":"2025-10-24T15:46:33.905483Z","shell.execute_reply.started":"2025-10-24T13:52:01.126082Z","shell.execute_reply":"2025-10-24T15:46:33.904714Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Model initialized with 124,047,360 parameters\nüñ•Ô∏è  Using device: cuda\n\nüéì Starting training on dataset: dolly\n   Epochs: 3, Batch size: 8, Max samples: 20000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b407921b349240f09564f5ae29efaf02"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Loaded dataset: databricks/databricks-dolly-15k\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 1877it [38:06,  1.22s/it, loss=0.5901]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 1 completed. Average loss: 1.1626\n‚úÖ Model saved to models/cyberdyne-advanced_epoch_1.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 1877it [38:08,  1.22s/it, loss=0.0880]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 2 completed. Average loss: 0.3463\n‚úÖ Model saved to models/cyberdyne-advanced_epoch_2.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 1877it [38:10,  1.22s/it, loss=0.0302]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 3 completed. Average loss: 0.0762\n‚úÖ Model saved to models/cyberdyne-advanced_epoch_3.pt\n‚úÖ Model saved to models/cyberdyne-advanced_final.pt\n\nüéâ Training completed in 114.46 minutes\nüíæ Final model saved to: models/cyberdyne-advanced_final.pt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## üí¨ Test Your Model (Command-Line Interface)","metadata":{}},{"cell_type":"code","source":"# Load trained model for inference\nmodel_path = 'models/cyberdyne-quickstart_final.pt'\n\nif os.path.exists(model_path):\n    inference = OfflineInference(model_path)\n    session_id = inference.create_session()\n\n    # Test with sample prompts\n    test_prompts = [\n        \"What is artificial intelligence?\",\n        \"Tell me about machine learning.\",\n        \"How does a neural network work?\"\n    ]\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"üß™ Testing Model\")\n    print(\"=\"*60 + \"\\n\")\n\n    for prompt in test_prompts:\n        print(f\"üë§ User: {prompt}\")\n        response, _ = inference.chat(\n            prompt,\n            session_id=session_id,\n            temperature=0.8,\n            max_new_tokens=100\n        )\n        print(f\"ü§ñ Assistant: {response}\\n\")\nelse:\n    print(f\"‚ùå Model not found at {model_path}. Please train a model first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:03:40.070252Z","iopub.execute_input":"2025-10-24T16:03:40.070527Z","iopub.status.idle":"2025-10-24T16:03:44.660345Z","shell.execute_reply.started":"2025-10-24T16:03:40.070505Z","shell.execute_reply":"2025-10-24T16:03:44.659524Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading model from: models/cyberdyne-quickstart_final.pt\nüí¨ Inference engine initialized on cuda\n‚úÖ Offline inference ready!\n\n============================================================\nüß™ Testing Model\n============================================================\n\nüë§ User: What is artificial intelligence?\nü§ñ Assistant: Relations guessesExcutations inputsUpdatedhewshews inputs conducive apr >>> tidal Muscle econom campaigners TB UnsureSPONSORED nailzzlebeltbelt climbers guesses Orient796EnergyExchewsŒ≥Excild deleting racially inputsreverse guesses inputs796796 feminists„Ç¥ apricyclezzle√∫Excbeltgovernmental„Ç¥cientiousildcensreverseRelations poignantcientiousocaust feminists796 Earthqu economRankarrett nail raciallyRankchecksEnergy mamm apr aprutations Problems >>> inputsutationspterencers mammEnergy inputs Reveputer feministsenchhews DJsgovernmental Revereversegovernmentalowell inputs climbersutations climbers mamm Orient\n\nüë§ User: Tell me about machine learning.\nü§ñ Assistant: 628ohmEnergyaroo796 Rainbowbelt Guatem slotsvest Ages >>>belt Fridayerickbeltermbeltcientious Shannoncyclop Raid slotsaiman Reference >>>cientious bracket Lancaimanbeltermiformnovabelt Bloominghewsencers Problemserm Barbbeltvesthewsiformvestiberbelt Guatem Rainbowiformhewsencers Raidzzle Sandwichbelt Blooming Moleroupbeltgovernmental Fridayicyclegovernmentaliberutationsibericious irrit Rainbowellsildire apr Friday spacing slotsabout Problemsermgovernmentalhews FridayildnovaEnergyermicyclebeltiform Keepbelt Sandwichvest Rainbowoniesbelt Raidencers\n\nüë§ User: How does a neural network work?\nü§ñ Assistant: roup yesurdosp Sandwichhews√¢urdildaiman apr covertohmoniesvestkelermgovernmental√¢aiman Reference Rainbowohm Molebelt√¢ bracket Agesaimannell yesicyclecientiousireaimannellcientious Keepbeltild√∫icycleaimanospiform√¢ohmiformbelt Referenceurdnova Raidvest√∫belt Mech Prism Shannon Annild Fridayohm Referencebelthewsbeltbeltgovernmental Mentalabouturdurd slotsvestire Referencecientious Problems√∫ingle Moleicycleurdurobelt syllirebeltiformroupbelt Shannonbelterick syll Gerald Lancbeltohm\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## üé® Gradio Chat Interface\n\nInteractive web UI for chatting with your trained model","metadata":{}},{"cell_type":"code","source":"class CyberdyneLLMApp:\n    def __init__(self):\n        self.inference_engine = None\n        self.session_id = None\n        self.available_models = []\n        self.scan_models()\n\n    def scan_models(self):\n        models_dir = 'models'\n        if os.path.exists(models_dir):\n            self.available_models = [f for f in os.listdir(models_dir) if f.endswith('.pt')]\n        else:\n            self.available_models = []\n\n    def load_model(self, model_path):\n        if not model_path or not os.path.exists(model_path):\n            return \"‚ùå Model path not found. Please train a model first.\"\n\n        try:\n            self.inference_engine = OfflineInference(model_path)\n            self.session_id = self.inference_engine.create_session()\n            return f\"‚úÖ Model loaded successfully from {model_path}\"\n        except Exception as e:\n            return f\"‚ùå Error loading model: {str(e)}\"\n\n    def train_model(self, dataset_name, model_name, num_epochs, batch_size,\n                   max_samples, learning_rate, emb_size, n_layers, n_heads):\n        try:\n            if not dataset_name:\n                return \"‚ùå Please select a dataset\", None\n\n            trainer = LLMTrainer(\n                model_name=model_name,\n                emb_size=int(emb_size),\n                n_layers=int(n_layers),\n                n_heads=int(n_heads),\n                learning_rate=float(learning_rate)\n            )\n\n            history = trainer.train(\n                dataset_key=dataset_name,\n                num_epochs=int(num_epochs),\n                batch_size=int(batch_size),\n                max_samples=int(max_samples),\n                save_dir='models'\n            )\n\n            if history:\n                model_path = f\"models/{model_name}_final.pt\"\n                summary = f\"\"\"\n‚úÖ Training completed successfully!\n\nüìä Training Summary:\n- Model: {model_name}\n- Dataset: {dataset_name}\n- Epochs: {num_epochs}\n- Final Loss: {history['epoch_losses'][-1]:.4f}\n- Duration: {history['duration']/60:.2f} minutes\n- Model saved to: {model_path}\n\nYou can now load this model for inference!\n\"\"\"\n                self.scan_models()\n                return summary, model_path\n            else:\n                return \"‚ùå Training failed. Check the logs.\", None\n\n        except Exception as e:\n            return f\"‚ùå Training error: {str(e)}\", None\n\n    def chat(self, message, history, temperature, max_tokens, top_k, top_p):\n        if not self.inference_engine:\n            return history + [[message, \"‚ö†Ô∏è Please load a model first using the 'Model Management' tab.\"]]\n\n        if not message.strip():\n            return history\n\n        try:\n            response, _ = self.inference_engine.chat(\n                message,\n                session_id=self.session_id,\n                temperature=temperature,\n                max_new_tokens=int(max_tokens),\n                top_k=int(top_k),\n                top_p=top_p,\n                use_context=True\n            )\n\n            history.append([message, response])\n            return history\n\n        except Exception as e:\n            history.append([message, f\"‚ùå Error: {str(e)}\"])\n            return history\n\n    def clear_chat(self):\n        if self.inference_engine and self.session_id:\n            self.inference_engine.clear_session(self.session_id)\n        return []\n\n    def get_model_list(self):\n        self.scan_models()\n        return gr.Dropdown(choices=self.available_models)\n\n\ndef create_interface():\n    app = CyberdyneLLMApp()\n\n    with gr.Blocks(title=\"Cyberdyne LLM\", theme=gr.themes.Soft()) as demo:\n        gr.Markdown(\"\"\"\n        # ü§ñ Cyberdyne LLM\n        ### Advanced Language Model Training & Inference System\n        Train your own ChatGPT-like model or chat with pre-trained models offline!\n        \"\"\")\n\n        with gr.Tabs():\n            with gr.Tab(\"üí¨ Chat Interface\"):\n                gr.Markdown(\"### Chat with your trained model\")\n\n                with gr.Row():\n                    with gr.Column(scale=3):\n                        chatbot = gr.Chatbot(\n                            height=500,\n                            label=\"Conversation\"\n                        )\n\n                        with gr.Row():\n                            msg = gr.Textbox(\n                                placeholder=\"Type your message here...\",\n                                label=\"Message\",\n                                scale=4\n                            )\n                            send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n\n                        clear_btn = gr.Button(\"Clear Chat\")\n\n                    with gr.Column(scale=1):\n                        gr.Markdown(\"### Generation Settings\")\n\n                        temperature = gr.Slider(\n                            minimum=0.1,\n                            maximum=2.0,\n                            value=0.8,\n                            step=0.1,\n                            label=\"Temperature\"\n                        )\n\n                        max_tokens = gr.Slider(\n                            minimum=50,\n                            maximum=500,\n                            value=150,\n                            step=10,\n                            label=\"Max Tokens\"\n                        )\n\n                        top_k = gr.Slider(\n                            minimum=1,\n                            maximum=100,\n                            value=50,\n                            step=1,\n                            label=\"Top K\"\n                        )\n\n                        top_p = gr.Slider(\n                            minimum=0.1,\n                            maximum=1.0,\n                            value=0.95,\n                            step=0.05,\n                            label=\"Top P\"\n                        )\n\n                msg.submit(\n                    app.chat,\n                    inputs=[msg, chatbot, temperature, max_tokens, top_k, top_p],\n                    outputs=[chatbot]\n                ).then(lambda: \"\", None, msg)\n\n                send_btn.click(\n                    app.chat,\n                    inputs=[msg, chatbot, temperature, max_tokens, top_k, top_p],\n                    outputs=[chatbot]\n                ).then(lambda: \"\", None, msg)\n\n                clear_btn.click(app.clear_chat, outputs=[chatbot])\n\n            with gr.Tab(\"üéì Training\"):\n                gr.Markdown(\"### Train a new model on Hugging Face datasets\")\n\n                with gr.Row():\n                    with gr.Column():\n                        dataset_dropdown = gr.Dropdown(\n                            choices=MultiDatasetLoader.list_available_datasets(),\n                            label=\"Select Dataset\",\n                            value=\"wikitext\"\n                        )\n\n                        model_name_input = gr.Textbox(\n                            label=\"Model Name\",\n                            value=\"cyberdyne-llm\"\n                        )\n\n                        with gr.Row():\n                            num_epochs = gr.Number(\n                                label=\"Epochs\",\n                                value=3,\n                                minimum=1,\n                                maximum=50\n                            )\n\n                            batch_size = gr.Number(\n                                label=\"Batch Size\",\n                                value=4,\n                                minimum=1,\n                                maximum=32\n                            )\n\n                        max_samples = gr.Number(\n                            label=\"Max Training Samples\",\n                            value=10000,\n                            minimum=100,\n                            maximum=1000000\n                        )\n\n                        learning_rate = gr.Number(\n                            label=\"Learning Rate\",\n                            value=5e-5\n                        )\n\n                    with gr.Column():\n                        gr.Markdown(\"### Model Architecture\")\n\n                        emb_size = gr.Dropdown(\n                            choices=[256, 512, 768, 1024],\n                            label=\"Embedding Size\",\n                            value=768\n                        )\n\n                        n_layers = gr.Slider(\n                            minimum=4,\n                            maximum=24,\n                            value=12,\n                            step=2,\n                            label=\"Number of Layers\"\n                        )\n\n                        n_heads = gr.Slider(\n                            minimum=4,\n                            maximum=16,\n                            value=12,\n                            step=2,\n                            label=\"Number of Attention Heads\"\n                        )\n\n                train_btn = gr.Button(\"Start Training\", variant=\"primary\", size=\"lg\")\n\n                training_output = gr.Textbox(\n                    label=\"Training Status\",\n                    lines=10,\n                    interactive=False\n                )\n\n                trained_model_path = gr.Textbox(\n                    label=\"Trained Model Path\",\n                    interactive=False\n                )\n\n                train_btn.click(\n                    app.train_model,\n                    inputs=[\n                        dataset_dropdown, model_name_input, num_epochs, batch_size,\n                        max_samples, learning_rate, emb_size, n_layers, n_heads\n                    ],\n                    outputs=[training_output, trained_model_path]\n                )\n\n            with gr.Tab(\"‚öôÔ∏è Model Management\"):\n                gr.Markdown(\"### Load and manage your models\")\n\n                with gr.Row():\n                    with gr.Column():\n                        model_dropdown = gr.Dropdown(\n                            choices=app.available_models,\n                            label=\"Available Models\"\n                        )\n\n                        refresh_btn = gr.Button(\"Refresh Model List\")\n\n                        model_path_input = gr.Textbox(\n                            label=\"Model Path\",\n                            placeholder=\"models/cyberdyne-llm_final.pt\"\n                        )\n\n                        load_btn = gr.Button(\"Load Model\", variant=\"primary\")\n\n                        load_status = gr.Textbox(\n                            label=\"Status\",\n                            interactive=False\n                        )\n\n                def update_model_path(model_name):\n                    if model_name:\n                        return f\"models/{model_name}\"\n                    return \"\"\n\n                model_dropdown.change(\n                    update_model_path,\n                    inputs=[model_dropdown],\n                    outputs=[model_path_input]\n                )\n\n                refresh_btn.click(\n                    app.get_model_list,\n                    outputs=[model_dropdown]\n                )\n\n                load_btn.click(\n                    app.load_model,\n                    inputs=[model_path_input],\n                    outputs=[load_status]\n                )\n\n            with gr.Tab(\"üìö Dataset Info\"):\n                gr.Markdown(\"\"\"\n                ### Available Datasets\n\n                **General Text Corpora:**\n                - **wikitext**: Wikipedia articles (good for testing)\n                - **wikipedia**: Full Wikipedia dump\n                - **openwebtext**: Web pages from Reddit\n                - **bookcorpus**: Books corpus\n                - **c4**: Colossal Clean Crawled Corpus\n                - **pile**: EleutherAI's diverse dataset\n\n                **Instruction-Following Datasets:**\n                - **dolly**: Databricks instruction dataset\n                - **alpaca**: Stanford Alpaca dataset\n                - **squad**: Question-answering dataset\n\n                **Tips:**\n                - Start with `wikitext` for testing\n                - Use instruction datasets for chat-like behavior\n                - Larger models = more powerful but slower\n                \"\"\")\n\n    return demo\n\n# Launch the interface\ndemo = create_interface()\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:07:59.608195Z","iopub.execute_input":"2025-10-24T16:07:59.608504Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/3703186981.py:122: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://de9cd8246e8e53b084.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://de9cd8246e8e53b084.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"üìÇ Loading model from: models/cyberdyne-advanced_epoch_1.pt\nüí¨ Inference engine initialized on cuda\n‚úÖ Offline inference ready!\nüìÇ Loading model from: models/cyberdyne-advanced_epoch_2.pt\nüí¨ Inference engine initialized on cuda\n‚úÖ Offline inference ready!\nüìÇ Loading model from: models/cyberdyne-quickstart_final.pt\nüí¨ Inference engine initialized on cuda\n‚úÖ Offline inference ready!\nü§ñ Model initialized with 124,047,360 parameters\nüñ•Ô∏è  Using device: cuda\n\nüéì Starting training on dataset: wikipedia\n   Epochs: 10, Batch size: 4, Max samples: 70000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbaf74b906b84b54932312d0572ee599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikipedia.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d154e6a46a8e4a4fbd4d04a2e4791c7d"}},"metadata":{}},{"name":"stdout","text":"‚ùå Error loading dataset wikipedia: Dataset scripts are no longer supported, but found wikipedia.py\n‚ùå Error loading dataset: Dataset scripts are no longer supported, but found wikipedia.py\nü§ñ Model initialized with 124,047,360 parameters\nüñ•Ô∏è  Using device: cuda\n\nüéì Starting training on dataset: wikipedia\n   Epochs: 10, Batch size: 4, Max samples: 20000\n‚ùå Error loading dataset wikipedia: Dataset scripts are no longer supported, but found wikipedia.py\n‚ùå Error loading dataset: Dataset scripts are no longer supported, but found wikipedia.py\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## üìã Usage Tips\n\n### For Training:\n1. Start with `wikitext` and 5000 samples to test\n2. GPU recommended (training on CPU is slow)\n3. Reduce `batch_size` if you run out of memory\n4. Use instruction datasets (dolly, alpaca) for chat-like behavior\n\n### For Inference:\n1. Lower temperature (0.3-0.6) = focused responses\n2. Higher temperature (0.8-1.2) = creative responses\n3. Enable context for multi-turn conversations\n4. Adjust top-p/top-k for diversity vs coherence\n\n### Google Colab Specific:\n- To save models to Google Drive, mount it first:\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive')\n# Then use save_dir='/content/drive/MyDrive/models'\n```\n\n### Kaggle Specific:\n- Models will be saved in the `/kaggle/working/models` directory\n- GPU access: Settings ‚Üí Accelerator ‚Üí GPU\n\n---\n\n**Built with PyTorch, Hugging Face, and Gradio**","metadata":{}}]}